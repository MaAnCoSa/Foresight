{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d91d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72da1535",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORESIGHT_DIRECTORY = os.path.dirname(os.getcwd())\n",
    "DATA_RAW_DIRECTORY = \"/data/raw/\"\n",
    "DATA_INTERIM_DIRECTORY = \"/data/interim/\"\n",
    "DATA = \"num_monsters_3_combat_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9aeb8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = FORESIGHT_DIRECTORY + DATA_RAW_DIRECTORY + DATA\n",
    "NEW_PATH = FORESIGHT_DIRECTORY + DATA_INTERIM_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d43015e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the heavy CSV file.\n",
    "csv_path = 'path/to/your/file.csv'\n",
    "\n",
    "# Define the chunk size (for example, 100,000 rows).\n",
    "chunksize = 100000\n",
    "\n",
    "# Dictionary to accumulate chunks for each group (from 1 to 7 players).\n",
    "dfs_chunks = {i: [] for i in range(1, 8)}\n",
    "\n",
    "# Read the CSV file in chunks.\n",
    "for chunk in pd.read_csv(FILE_PATH, chunksize=chunksize):\n",
    "    # 1 player: we assume that for this row there is only information for player 1,\n",
    "    # so column pc2_class contains '-'.\n",
    "    df1_chunk = chunk[chunk['pc2_class'] == '-']\n",
    "    dfs_chunks[1].append(df1_chunk)\n",
    "    \n",
    "    # 2 players: pc2_class has data (is not '-') and pc3_class equals '-'\n",
    "    df2_chunk = chunk[(chunk['pc2_class'] != '-') & (chunk['pc3_class'] == '-')]\n",
    "    dfs_chunks[2].append(df2_chunk)\n",
    "    \n",
    "    # 3 players: pc3_class has data and pc4_class equals '-'\n",
    "    df3_chunk = chunk[(chunk['pc3_class'] != '-') & (chunk['pc4_class'] == '-')]\n",
    "    dfs_chunks[3].append(df3_chunk)\n",
    "    \n",
    "    # 4 players: pc4_class has data and pc5_class equals '-'\n",
    "    df4_chunk = chunk[(chunk['pc4_class'] != '-') & (chunk['pc5_class'] == '-')]\n",
    "    dfs_chunks[4].append(df4_chunk)\n",
    "    \n",
    "    # 5 players: pc5_class has data and pc6_class equals '-'\n",
    "    df5_chunk = chunk[(chunk['pc5_class'] != '-') & (chunk['pc6_class'] == '-')]\n",
    "    dfs_chunks[5].append(df5_chunk)\n",
    "    \n",
    "    # 6 players: pc6_class has data and pc7_class equals '-'\n",
    "    df6_chunk = chunk[(chunk['pc6_class'] != '-') & (chunk['pc7_class'] == '-')]\n",
    "    dfs_chunks[6].append(df6_chunk)\n",
    "    \n",
    "    # 7 players: we assume that in this row there is information in pc7_class\n",
    "    df7_chunk = chunk[chunk['pc7_class'] != '-']\n",
    "    dfs_chunks[7].append(df7_chunk)\n",
    "\n",
    "# Concatenate each list of chunks to obtain the final DataFrames.\n",
    "df1 = pd.concat(dfs_chunks[1], ignore_index=True) if dfs_chunks[1] else pd.DataFrame()\n",
    "df2 = pd.concat(dfs_chunks[2], ignore_index=True) if dfs_chunks[2] else pd.DataFrame()\n",
    "df3 = pd.concat(dfs_chunks[3], ignore_index=True) if dfs_chunks[3] else pd.DataFrame()\n",
    "df4 = pd.concat(dfs_chunks[4], ignore_index=True) if dfs_chunks[4] else pd.DataFrame()\n",
    "df5 = pd.concat(dfs_chunks[5], ignore_index=True) if dfs_chunks[5] else pd.DataFrame()\n",
    "df6 = pd.concat(dfs_chunks[6], ignore_index=True) if dfs_chunks[6] else pd.DataFrame()\n",
    "df7 = pd.concat(dfs_chunks[7], ignore_index=True) if dfs_chunks[7] else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d658c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminited the first column\n",
    "df1 = df1.iloc[:,1:]\n",
    "df2 = df2.iloc[:,1:]\n",
    "df3 = df3.iloc[:,1:]\n",
    "df4 = df4.iloc[:,1:]\n",
    "df5 = df5.iloc[:,1:]\n",
    "df6 = df6.iloc[:,1:]\n",
    "df7 = df7.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of your DataFrames (make sure df1 to df7 are already defined)\n",
    "dfs = [df1, df2, df3, df4, df5, df6, df7]\n",
    "\n",
    "# Column index ranges to drop for each DataFrame\n",
    "start_indices = [10, 20, 30, 40, 50, 60, 70]\n",
    "\n",
    "# Apply the column drop to each DataFrame\n",
    "for i, start in enumerate(start_indices):\n",
    "    drop_columns = list(range(start, 70))  # Up to column 70 (exclusive)\n",
    "    columns_to_drop = df1.columns[drop_columns]  # Use df1 as reference for column names\n",
    "    dfs[i] = dfs[i].drop(columns=columns_to_drop)\n",
    "    dfs[i].to_csv(NEW_PATH + f\"players_{i+1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870e6a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95284, 144)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1.pc1_class == 'Bard'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
